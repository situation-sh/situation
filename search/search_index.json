{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Situation provides the core infrastructure to automatically collect and consolidate IT data (machines, device, apps, network, flows...), on its own. Providing then an up-to-date and reliable view of the current state of your infra (or your home LAN), namely the graph . Now you are ready to build a context-rich IT tool above Situation. Yet another scanning tool? Situation is different from common tools like nmap , telegraf or osquery : It aims to run without prior knowledge Agents collaborate natively It builds the whole infra, namely the graph (not only the nodes)","title":"Situation"},{"location":"01_quick_start/","text":"Installation The agent currently supports Linux ( armv5 , armv6 , armv7 , arm64 and amd64 ) and Windows (only amd64 ). The binaries are made available through github releases . You can also compile it from sources (once you have have a go compiler >=1.18 ): go install github.com/situation-sh/situation/agent Quick run You can run the agent without data persistence (in-memory database) Linux situation run Windows situation . exe run If you want to output an sqlite database, just add the --db flag Linux situation run --db = situation.sqlite Windows situation . exe run - -db = situation . sqlite Exploring (experimental) Situation embeds a minimal terminal ui (tui) that briefly shows the collected data. It can pop up after the run with the --explore flag, Linux situation run --explore Windows situation . exe run - -explore or the explore command can also be used in case of data persistence Linux situation explore --db = situation.sqlite Windows situation . exe explore - -db = situation . sqlite Cooperation Here is where the IT data collection platform starts! You can let the agents cooperate by providing them a common postgres database: Linux situation run --db = postgresql:// [ user ] : [ password ] @ [ host ] : [ port ] / [ database ] Windows situation . exe run - -db = postgresql :// [user] : [password] @ [host] : [port] / [database]","title":"Quick start"},{"location":"01_quick_start/#installation","text":"The agent currently supports Linux ( armv5 , armv6 , armv7 , arm64 and amd64 ) and Windows (only amd64 ). The binaries are made available through github releases . You can also compile it from sources (once you have have a go compiler >=1.18 ): go install github.com/situation-sh/situation/agent","title":"Installation"},{"location":"01_quick_start/#quick-run","text":"You can run the agent without data persistence (in-memory database) Linux situation run Windows situation . exe run If you want to output an sqlite database, just add the --db flag Linux situation run --db = situation.sqlite Windows situation . exe run - -db = situation . sqlite","title":"Quick run"},{"location":"01_quick_start/#exploring-experimental","text":"Situation embeds a minimal terminal ui (tui) that briefly shows the collected data. It can pop up after the run with the --explore flag, Linux situation run --explore Windows situation . exe run - -explore or the explore command can also be used in case of data persistence Linux situation explore --db = situation.sqlite Windows situation . exe explore - -db = situation . sqlite","title":"Exploring (experimental)"},{"location":"01_quick_start/#cooperation","text":"Here is where the IT data collection platform starts! You can let the agents cooperate by providing them a common postgres database: Linux situation run --db = postgresql:// [ user ] : [ password ] @ [ host ] : [ port ] / [ database ] Windows situation . exe run - -db = postgresql :// [user] : [password] @ [host] : [port] / [database]","title":"Cooperation"},{"location":"02_setup/","text":"One of the feature of Situation is that agents can collaborate once they share the same database (currently, only postgres is supported). Database The first step is to provision a PostgreSQL database. You can choose to host an instance on-premise (with docker for instance) or let a remote service do it for you (like supabase or any cloud provider). Here is an example with docker: docker run \\ --rm \\ --detach \\ --name situation-pg \\ -e \"POSTGRES_PASSWORD=password\" \\ -e \"POSTGRES_USER=user\" \\ -e \"POSTGRES_DB=situation\" \\ -p \"5432:5432\" \\ --health-cmd = \"pg_isready -U user\" \\ --health-interval = 2s \\ --health-timeout = 2s \\ --health-retries = 2 \\ postgres:17.6 The paramount thing is to get the DSN of the instance so as to pass it to the agents. In the example above it is something like postgres://user:password@[ENDPOINT]/situation?sslmode=disable . Agents Once your db is ready, you can deploy agents anywhere on your infra. We advise to do it incrementally since IT mapping generally means IT exploring . Rule of thumbs deploy one agent per subnet since every agent is able to grab data from neighboring hosts deploy agent on hosts where exhaustive data collection is mandatory On each host where agent is deployed, follow the next steps. Identifier Every agent has an internal ID that must be unique across your swarm. Stock agent has a default identifier that must be changed. Linux situation refresh-id Windows situation . exe refresh-id Important: Once the agent has its unique ID, it must keep it and stay on the same host. You can check the new id by calling the id subcommand. Execution Linux situation run --db = \"postgres://user:password@[ENDPOINT]/situation?sslmode=disable\" Windows situation . exe run - -db = \"postgres://user:password@[ENDPOINT]/situation?sslmode=disable\"","title":"Setup"},{"location":"02_setup/#database","text":"The first step is to provision a PostgreSQL database. You can choose to host an instance on-premise (with docker for instance) or let a remote service do it for you (like supabase or any cloud provider). Here is an example with docker: docker run \\ --rm \\ --detach \\ --name situation-pg \\ -e \"POSTGRES_PASSWORD=password\" \\ -e \"POSTGRES_USER=user\" \\ -e \"POSTGRES_DB=situation\" \\ -p \"5432:5432\" \\ --health-cmd = \"pg_isready -U user\" \\ --health-interval = 2s \\ --health-timeout = 2s \\ --health-retries = 2 \\ postgres:17.6 The paramount thing is to get the DSN of the instance so as to pass it to the agents. In the example above it is something like postgres://user:password@[ENDPOINT]/situation?sslmode=disable .","title":"Database"},{"location":"02_setup/#agents","text":"Once your db is ready, you can deploy agents anywhere on your infra. We advise to do it incrementally since IT mapping generally means IT exploring . Rule of thumbs deploy one agent per subnet since every agent is able to grab data from neighboring hosts deploy agent on hosts where exhaustive data collection is mandatory On each host where agent is deployed, follow the next steps.","title":"Agents"},{"location":"02_setup/#identifier","text":"Every agent has an internal ID that must be unique across your swarm. Stock agent has a default identifier that must be changed. Linux situation refresh-id Windows situation . exe refresh-id Important: Once the agent has its unique ID, it must keep it and stay on the same host. You can check the new id by calling the id subcommand.","title":"Identifier"},{"location":"02_setup/#execution","text":"Linux situation run --db = \"postgres://user:password@[ENDPOINT]/situation?sslmode=disable\" Windows situation . exe run - -db = \"postgres://user:password@[ENDPOINT]/situation?sslmode=disable\"","title":"Execution"},{"location":"03_security/","text":"This page covers security hardening for a PostgreSQL-backed setup where multiple agents share the same database. It only applies to the PostgreSQL backend \u2014 SQLite deployments are out of scope. The following topics are covered: TLS \u2014 encrypting connections and verifying server/client identity Roles \u2014 limiting agent privileges to read/write only Feel free to select and adapt what applies to your environment. In all cases, refer to the official PostgreSQL administration documentation . Assumptions A superuser already exists (e.g. user / password ). In Docker, this is set via POSTGRES_USER and POSTGRES_PASSWORD . Data is stored in the situation schema, in a database named situation (Docker: POSTGRES_DB ). The instance is reachable at db.example.org:5432 . TLS TLS can provide connection encryption as well as server and client authentication. Enable SSL mode Enabling SSL is the first step to use TLS features. According to the postgres documentation: To start in SSL mode, files containing the server certificate and private key must exist. By default, these files are expected to be named server.crt and server.key, respectively, in the server's data directory 1 On postgresql.conf you can then enable SSL as follows, assuming that you already provide server.crt and server.key at the right location ( $PGDATA folder). ssl = on # ssl_cert_file is default to '${PGDATA}/server.crt' # ssl_key_file is default to '${PGDATA}/server.key' Reject unencrypted connections Through the pg_hba.conf file, the server can reject any remote connections that do not use SSL. # Trust local (local connection can be made unconditionally) # conntype db user auth-method local all all trust # Allow ssl connections # conntype db user address auth-method hostssl all all 0.0.0.0/0 scram-sha-256 hostssl all all : :/0 scram-sha-256 # Explicitly reject non-SSL TCP connections # conntype db user address auth-method hostnossl all all 0.0.0.0/0 reject hostnossl all all : :/0 reject On the client side, at minimum sslmode=require must be passed to ensure that nothing will be sent unencrypted. Using sslmode=verify-full is recommended to also verify the server identity (see Client verification below). Client verification By default, PostgreSQL will not perform any verification of the server certificate. This means that it is possible to spoof the server identity without the client knowing. 2 To let agents (or other third-party clients) verifying server's identity we must provide to them the certificate authority that signed the server certificate. Currently the CA certificate cannot be inlined in the DSN, so you must put it somewhere on the system where the client runs and append sslmode=verify-full&sslrootcert=/path/to/ca.crt to the DSN. Here is an example of the db migration made by the agent. Linux situation migrate --db = \"postgres://user:password@db.example.org:5432/situation?sslmode=verify-full&sslrootcert=ca.crt\" Windows situation . exe migrate - -db = \"postgres://user:password@db.example.org:5432/situation?sslmode=verify-full&sslrootcert=ca.crt\" Note Setting sslmode=verify-ca will check the server certificate (i.e. it is well signed by the CA) but it won't verify that the server host name matches the name stored in the server certificate. Client certificate The PostgreSQL server can also authenticate client through SSL certificates (in SSL connections only of course). You can modify pg_hba.conf as follows to activate this method. # Allow ssl connections with client cert auth # conntype db user address auth-method hostssl all all 0.0.0.0/0 cert hostssl all all : :/0 cert According to the docs: The CN (Common Name) attribute of the certificate will be compared to the requested database user name 3 So the server will check whether the client cert is well signed by the trusted CA and if its CN matches the user in the DSN. On the postgresql.conf you should append: # trusted certificate authorities ssl_ca_file = /path/to/ca.crt On the client side, client certificate and private key must be passed through the sslcert and sslkey options (see the example below). The password can then be omitted. Linux situation migrate --db = \"postgres://user:@db.example.org:5432/situation?sslmode=verify-full&sslrootcert=ca.crt&sslcert=client.crt&sslkey=client.key\" Windows situation . exe migrate - -db = \"postgres://user:@db.example.org:5432/situation?sslmode=verify-full&sslrootcert=ca.crt&sslcert=client.crt&sslkey=client.key\" Warning Some options like sslcert or sslkey are not supported by postgres clients. Roles We can also add roles to limit privileges. For instance, we can keep the superuser role to run migrations and then create a basic agent role that will only perform read/write and won't be able to modify tables. In the postgres instance: -- Agents: read/write data only, cannot touch schema CREATE ROLE agent LOGIN PASSWORD 'secure-password' ; GRANT USAGE ON SCHEMA situation TO agent ; Assuming migrations have been run with the superuser (or another user with enough privileges), you can run the agent with --no-migration so that it does not try to run the migrations. Linux situation run --no-migration --db = \"postgres://agent:secure-password@db.example.org:5432/situation?...\" Windows situation . exe run - -no-migration - -db = \"postgres://agent:secure-password@db.example.org:5432/situation?...\" https://www.postgresql.org/docs/current/ssl-tcp.html#SSL-SETUP \u21a9 https://www.postgresql.org/docs/current/libpq-ssl.html#LIBPQ-SSL \u21a9 https://www.postgresql.org/docs/current/auth-cert.html#AUTH-CERT \u21a9","title":"Security"},{"location":"03_security/#tls","text":"TLS can provide connection encryption as well as server and client authentication.","title":"TLS"},{"location":"03_security/#enable-ssl-mode","text":"Enabling SSL is the first step to use TLS features. According to the postgres documentation: To start in SSL mode, files containing the server certificate and private key must exist. By default, these files are expected to be named server.crt and server.key, respectively, in the server's data directory 1 On postgresql.conf you can then enable SSL as follows, assuming that you already provide server.crt and server.key at the right location ( $PGDATA folder). ssl = on # ssl_cert_file is default to '${PGDATA}/server.crt' # ssl_key_file is default to '${PGDATA}/server.key'","title":"Enable SSL mode"},{"location":"03_security/#reject-unencrypted-connections","text":"Through the pg_hba.conf file, the server can reject any remote connections that do not use SSL. # Trust local (local connection can be made unconditionally) # conntype db user auth-method local all all trust # Allow ssl connections # conntype db user address auth-method hostssl all all 0.0.0.0/0 scram-sha-256 hostssl all all : :/0 scram-sha-256 # Explicitly reject non-SSL TCP connections # conntype db user address auth-method hostnossl all all 0.0.0.0/0 reject hostnossl all all : :/0 reject On the client side, at minimum sslmode=require must be passed to ensure that nothing will be sent unencrypted. Using sslmode=verify-full is recommended to also verify the server identity (see Client verification below).","title":"Reject unencrypted connections"},{"location":"03_security/#client-verification","text":"By default, PostgreSQL will not perform any verification of the server certificate. This means that it is possible to spoof the server identity without the client knowing. 2 To let agents (or other third-party clients) verifying server's identity we must provide to them the certificate authority that signed the server certificate. Currently the CA certificate cannot be inlined in the DSN, so you must put it somewhere on the system where the client runs and append sslmode=verify-full&sslrootcert=/path/to/ca.crt to the DSN. Here is an example of the db migration made by the agent. Linux situation migrate --db = \"postgres://user:password@db.example.org:5432/situation?sslmode=verify-full&sslrootcert=ca.crt\" Windows situation . exe migrate - -db = \"postgres://user:password@db.example.org:5432/situation?sslmode=verify-full&sslrootcert=ca.crt\" Note Setting sslmode=verify-ca will check the server certificate (i.e. it is well signed by the CA) but it won't verify that the server host name matches the name stored in the server certificate.","title":"Client verification"},{"location":"03_security/#client-certificate","text":"The PostgreSQL server can also authenticate client through SSL certificates (in SSL connections only of course). You can modify pg_hba.conf as follows to activate this method. # Allow ssl connections with client cert auth # conntype db user address auth-method hostssl all all 0.0.0.0/0 cert hostssl all all : :/0 cert According to the docs: The CN (Common Name) attribute of the certificate will be compared to the requested database user name 3 So the server will check whether the client cert is well signed by the trusted CA and if its CN matches the user in the DSN. On the postgresql.conf you should append: # trusted certificate authorities ssl_ca_file = /path/to/ca.crt On the client side, client certificate and private key must be passed through the sslcert and sslkey options (see the example below). The password can then be omitted. Linux situation migrate --db = \"postgres://user:@db.example.org:5432/situation?sslmode=verify-full&sslrootcert=ca.crt&sslcert=client.crt&sslkey=client.key\" Windows situation . exe migrate - -db = \"postgres://user:@db.example.org:5432/situation?sslmode=verify-full&sslrootcert=ca.crt&sslcert=client.crt&sslkey=client.key\" Warning Some options like sslcert or sslkey are not supported by postgres clients.","title":"Client certificate"},{"location":"03_security/#roles","text":"We can also add roles to limit privileges. For instance, we can keep the superuser role to run migrations and then create a basic agent role that will only perform read/write and won't be able to modify tables. In the postgres instance: -- Agents: read/write data only, cannot touch schema CREATE ROLE agent LOGIN PASSWORD 'secure-password' ; GRANT USAGE ON SCHEMA situation TO agent ; Assuming migrations have been run with the superuser (or another user with enough privileges), you can run the agent with --no-migration so that it does not try to run the migrations. Linux situation run --no-migration --db = \"postgres://agent:secure-password@db.example.org:5432/situation?...\" Windows situation . exe run - -no-migration - -db = \"postgres://agent:secure-password@db.example.org:5432/situation?...\" https://www.postgresql.org/docs/current/ssl-tcp.html#SSL-SETUP \u21a9 https://www.postgresql.org/docs/current/libpq-ssl.html#LIBPQ-SSL \u21a9 https://www.postgresql.org/docs/current/auth-cert.html#AUTH-CERT \u21a9","title":"Roles"},{"location":"05_CLI/","text":"Here is what the agent can do in addition to collecting data. Command Description run Run the agent explore Run the experimental terminal UI refresh-id Regenerate the internal ID of the agent defaults , def Print the default config id Print the identifier of the agent update Update the agent version Print the version of the agent task , cron Install a scheduled task help , h Shows a list of commands or help for one command Agent identifier Every agent binary can be identified through a 16 bytes id ( fc097e65503cb3ad9eb8e10f5a617611 by default). Currently you can't see this id in the database. In the future, it may be present in an attribute like updated_by . You can display the current id through the eponym command. Linux situation id Windows situation . exe id In different scenarios you may need to customize this id (naming, multi-deployment...). For these purpioses, you can generate a new random ID (or provide a new one in hex format): Linux situation refresh-id Windows situation . exe refresh-id Run configuration By design, you can run the agent as-is but it is also possible to tune modules. Module configuration Some modules may expose specific option through flags. Do not hesitate to look at them in the help. For example: Linux situation run --ping-timeout = 1s Windows situation . exe run - -ping-timeout = 1s Disabling modules All the module can be disabled through the following pattern --no-module-<module-name> (see the list of available modules ) Note As some modules may depend on others, disabling a module may lead to a cascasding effect. To force modules that depend on it to run, you must pass the --ignore-missing-deps flag. situation run --no-module-ping --ignore-missing-deps","title":"CLI"},{"location":"05_CLI/#agent-identifier","text":"Every agent binary can be identified through a 16 bytes id ( fc097e65503cb3ad9eb8e10f5a617611 by default). Currently you can't see this id in the database. In the future, it may be present in an attribute like updated_by . You can display the current id through the eponym command. Linux situation id Windows situation . exe id In different scenarios you may need to customize this id (naming, multi-deployment...). For these purpioses, you can generate a new random ID (or provide a new one in hex format): Linux situation refresh-id Windows situation . exe refresh-id","title":"Agent identifier"},{"location":"05_CLI/#run-configuration","text":"By design, you can run the agent as-is but it is also possible to tune modules.","title":"Run configuration"},{"location":"05_CLI/#module-configuration","text":"Some modules may expose specific option through flags. Do not hesitate to look at them in the help. For example: Linux situation run --ping-timeout = 1s Windows situation . exe run - -ping-timeout = 1s","title":"Module configuration"},{"location":"05_CLI/#disabling-modules","text":"All the module can be disabled through the following pattern --no-module-<module-name> (see the list of available modules ) Note As some modules may depend on others, disabling a module may lead to a cascasding effect. To force modules that depend on it to run, you must pass the --ignore-missing-deps flag. situation run --no-module-ping --ignore-missing-deps","title":"Disabling modules"},{"location":"10_roadmap/","text":"Data exhaustiveness is crucial so the main axis is to develop more modules. You can have a look to the ongoing issues .","title":"Roadmap"},{"location":"database/postgres/","text":"Primary Key\u2003 Foreign Key\u2003 Unique subnetworks Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ network_cidr VARCHAR network_addr VARCHAR mask_size BIGINT ip_version BIGINT gateway VARCHAR vlan_id BIGINT tag VARCHAR machines Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ hostname VARCHAR host_id VARCHAR arch VARCHAR platform VARCHAR distribution VARCHAR distribution_version VARCHAR distribution_family VARCHAR uptime BIGINT agent VARCHAR cpe VARCHAR chassis VARCHAR parent_machine_id BIGINT cpus Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ model_name VARCHAR vendor VARCHAR cores BIGINT machine_id BIGINT gpus Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ index BIGINT product VARCHAR vendor VARCHAR driver VARCHAR machine_id BIGINT disks Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ name VARCHAR model VARCHAR size BIGINT type VARCHAR controller VARCHAR partitions JSON machine_id BIGINT network_interfaces Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ name VARCHAR mac VARCHAR mac_vendor VARCHAR ip VARCHAR[] gateway VARCHAR flags JSON tag VARCHAR machine_id BIGINT network_interface_subnets Name Type network_interface_id BIGINT subnetwork_id BIGINT ip VARCHAR mac_subnet VARCHAR packages Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ name VARCHAR version VARCHAR vendor VARCHAR manager VARCHAR install_time_unix BIGINT files JSONB machine_id BIGINT applications Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ name VARCHAR args JSONB pid BIGINT version VARCHAR protocol VARCHAR config JSON cpe VARCHAR machine_id BIGINT package_id BIGINT application_endpoints Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ port INTEGER protocol VARCHAR addr VARCHAR tls JSON fingerprints JSON application_protocols JSONB saas VARCHAR application_id BIGINT network_interface_id BIGINT users Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ uid VARCHAR gid VARCHAR name VARCHAR username VARCHAR domain VARCHAR machine_id BIGINT user_applications Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ user_id BIGINT application_id BIGINT linux VARCHAR flows Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ src_application_id BIGINT src_network_interface_id BIGINT src_addr VARCHAR dst_endpoint_id BIGINT endpoint_policies Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ endpoint_id BIGINT action VARCHAR src_endpoint_id BIGINT src_addr VARCHAR priority BIGINT source VARCHAR","title":"PostgreSQL"},{"location":"database/postgres/#subnetworks","text":"Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ network_cidr VARCHAR network_addr VARCHAR mask_size BIGINT ip_version BIGINT gateway VARCHAR vlan_id BIGINT tag VARCHAR","title":"subnetworks"},{"location":"database/postgres/#machines","text":"Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ hostname VARCHAR host_id VARCHAR arch VARCHAR platform VARCHAR distribution VARCHAR distribution_version VARCHAR distribution_family VARCHAR uptime BIGINT agent VARCHAR cpe VARCHAR chassis VARCHAR parent_machine_id BIGINT","title":"machines"},{"location":"database/postgres/#cpus","text":"Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ model_name VARCHAR vendor VARCHAR cores BIGINT machine_id BIGINT","title":"cpus"},{"location":"database/postgres/#gpus","text":"Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ index BIGINT product VARCHAR vendor VARCHAR driver VARCHAR machine_id BIGINT","title":"gpus"},{"location":"database/postgres/#disks","text":"Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ name VARCHAR model VARCHAR size BIGINT type VARCHAR controller VARCHAR partitions JSON machine_id BIGINT","title":"disks"},{"location":"database/postgres/#network_interfaces","text":"Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ name VARCHAR mac VARCHAR mac_vendor VARCHAR ip VARCHAR[] gateway VARCHAR flags JSON tag VARCHAR machine_id BIGINT","title":"network_interfaces"},{"location":"database/postgres/#network_interface_subnets","text":"Name Type network_interface_id BIGINT subnetwork_id BIGINT ip VARCHAR mac_subnet VARCHAR","title":"network_interface_subnets"},{"location":"database/postgres/#packages","text":"Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ name VARCHAR version VARCHAR vendor VARCHAR manager VARCHAR install_time_unix BIGINT files JSONB machine_id BIGINT","title":"packages"},{"location":"database/postgres/#applications","text":"Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ name VARCHAR args JSONB pid BIGINT version VARCHAR protocol VARCHAR config JSON cpe VARCHAR machine_id BIGINT package_id BIGINT","title":"applications"},{"location":"database/postgres/#application_endpoints","text":"Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ port INTEGER protocol VARCHAR addr VARCHAR tls JSON fingerprints JSON application_protocols JSONB saas VARCHAR application_id BIGINT network_interface_id BIGINT","title":"application_endpoints"},{"location":"database/postgres/#users","text":"Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ uid VARCHAR gid VARCHAR name VARCHAR username VARCHAR domain VARCHAR machine_id BIGINT","title":"users"},{"location":"database/postgres/#user_applications","text":"Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ user_id BIGINT application_id BIGINT linux VARCHAR","title":"user_applications"},{"location":"database/postgres/#flows","text":"Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ src_application_id BIGINT src_network_interface_id BIGINT src_addr VARCHAR dst_endpoint_id BIGINT","title":"flows"},{"location":"database/postgres/#endpoint_policies","text":"Name Type id BIGINT created_at TIMESTAMPTZ updated_at TIMESTAMPTZ endpoint_id BIGINT action VARCHAR src_endpoint_id BIGINT src_addr VARCHAR priority BIGINT source VARCHAR","title":"endpoint_policies"},{"location":"database/sqlite/","text":"Primary Key\u2003 Foreign Key\u2003 Unique subnetworks Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP network_cidr VARCHAR network_addr VARCHAR mask_size INTEGER ip_version INTEGER gateway VARCHAR vlan_id INTEGER tag VARCHAR machines Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP hostname VARCHAR host_id VARCHAR arch VARCHAR platform VARCHAR distribution VARCHAR distribution_version VARCHAR distribution_family VARCHAR uptime INTEGER agent VARCHAR cpe VARCHAR chassis VARCHAR parent_machine_id INTEGER cpus Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP model_name VARCHAR vendor VARCHAR cores INTEGER machine_id INTEGER gpus Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP index INTEGER product VARCHAR vendor VARCHAR driver VARCHAR machine_id INTEGER disks Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP name VARCHAR model VARCHAR size INTEGER type VARCHAR controller VARCHAR partitions VARCHAR machine_id INTEGER network_interfaces Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP name VARCHAR mac VARCHAR mac_vendor VARCHAR ip VARCHAR gateway VARCHAR flags VARCHAR tag VARCHAR machine_id INTEGER network_interface_subnets Name Type network_interface_id INTEGER subnetwork_id INTEGER ip VARCHAR mac_subnet VARCHAR packages Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP name VARCHAR version VARCHAR vendor VARCHAR manager VARCHAR install_time_unix INTEGER files VARCHAR machine_id INTEGER applications Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP name VARCHAR args VARCHAR pid INTEGER version VARCHAR protocol VARCHAR config VARCHAR cpe VARCHAR machine_id INTEGER package_id INTEGER application_endpoints Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP port INTEGER protocol VARCHAR addr VARCHAR tls VARCHAR fingerprints VARCHAR application_protocols VARCHAR saas VARCHAR application_id INTEGER network_interface_id INTEGER users Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP uid VARCHAR gid VARCHAR name VARCHAR username VARCHAR domain VARCHAR machine_id INTEGER user_applications Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP user_id INTEGER application_id INTEGER linux VARCHAR flows Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP src_application_id INTEGER src_network_interface_id INTEGER src_addr VARCHAR dst_endpoint_id INTEGER endpoint_policies Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP endpoint_id INTEGER action VARCHAR src_endpoint_id INTEGER src_addr VARCHAR priority INTEGER source VARCHAR","title":"SQLite"},{"location":"database/sqlite/#subnetworks","text":"Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP network_cidr VARCHAR network_addr VARCHAR mask_size INTEGER ip_version INTEGER gateway VARCHAR vlan_id INTEGER tag VARCHAR","title":"subnetworks"},{"location":"database/sqlite/#machines","text":"Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP hostname VARCHAR host_id VARCHAR arch VARCHAR platform VARCHAR distribution VARCHAR distribution_version VARCHAR distribution_family VARCHAR uptime INTEGER agent VARCHAR cpe VARCHAR chassis VARCHAR parent_machine_id INTEGER","title":"machines"},{"location":"database/sqlite/#cpus","text":"Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP model_name VARCHAR vendor VARCHAR cores INTEGER machine_id INTEGER","title":"cpus"},{"location":"database/sqlite/#gpus","text":"Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP index INTEGER product VARCHAR vendor VARCHAR driver VARCHAR machine_id INTEGER","title":"gpus"},{"location":"database/sqlite/#disks","text":"Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP name VARCHAR model VARCHAR size INTEGER type VARCHAR controller VARCHAR partitions VARCHAR machine_id INTEGER","title":"disks"},{"location":"database/sqlite/#network_interfaces","text":"Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP name VARCHAR mac VARCHAR mac_vendor VARCHAR ip VARCHAR gateway VARCHAR flags VARCHAR tag VARCHAR machine_id INTEGER","title":"network_interfaces"},{"location":"database/sqlite/#network_interface_subnets","text":"Name Type network_interface_id INTEGER subnetwork_id INTEGER ip VARCHAR mac_subnet VARCHAR","title":"network_interface_subnets"},{"location":"database/sqlite/#packages","text":"Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP name VARCHAR version VARCHAR vendor VARCHAR manager VARCHAR install_time_unix INTEGER files VARCHAR machine_id INTEGER","title":"packages"},{"location":"database/sqlite/#applications","text":"Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP name VARCHAR args VARCHAR pid INTEGER version VARCHAR protocol VARCHAR config VARCHAR cpe VARCHAR machine_id INTEGER package_id INTEGER","title":"applications"},{"location":"database/sqlite/#application_endpoints","text":"Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP port INTEGER protocol VARCHAR addr VARCHAR tls VARCHAR fingerprints VARCHAR application_protocols VARCHAR saas VARCHAR application_id INTEGER network_interface_id INTEGER","title":"application_endpoints"},{"location":"database/sqlite/#users","text":"Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP uid VARCHAR gid VARCHAR name VARCHAR username VARCHAR domain VARCHAR machine_id INTEGER","title":"users"},{"location":"database/sqlite/#user_applications","text":"Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP user_id INTEGER application_id INTEGER linux VARCHAR","title":"user_applications"},{"location":"database/sqlite/#flows","text":"Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP src_application_id INTEGER src_network_interface_id INTEGER src_addr VARCHAR dst_endpoint_id INTEGER","title":"flows"},{"location":"database/sqlite/#endpoint_policies","text":"Name Type id INTEGER created_at TIMESTAMP updated_at TIMESTAMP endpoint_id INTEGER action VARCHAR src_endpoint_id INTEGER src_addr VARCHAR priority INTEGER source VARCHAR","title":"endpoint_policies"},{"location":"deploy/ansible/","text":"Agents can be deployed automatically through Ansible . Given an inventory you can run the playbook given below. ansible-playbook situation.yml -i inventory.yml The playbook can be tuned to customize where the agent will be installed. First it downloads locally the latest linux and windows binaries. Then it deploys the binaries to the hosts defined in the inventory and outputs a list of agents deployed. # situation.yml - name : Download latest binaries (once) hosts : localhost gather_facts : false vars : repo : \"situation-sh/situation\" download_dir : \"{{ TMP_DIR | default('/tmp/situation') }}\" tasks : - name : Ensure download directory exists file : path : \"{{ download_dir }}\" state : directory - name : Get latest release from GitHub API uri : url : \"https://api.github.com/repos/{{ repo }}/releases/latest\" return_content : true headers : Accept : \"application/vnd.github+json\" register : latest_release - name : Download matched release assets get_url : url : \"{{ item.browser_download_url }}\" dest : \"{{ download_dir }}/{{ item.name }}\" mode : \"0644\" force : false loop : \"{{ latest_release.json.assets }}\" when : \"'-linux' in item.name or '-windows.exe' in item.name\" - name : Deploy agents to hosts tags : deploy hosts : all gather_facts : false vars : src_dir : \"{{ TMP_DIR | default('/tmp/situation') }}\" dest_linux : \"{{ ansible_env.HOME }}/situation\" dest_windows : \"{{ ansible_env.USERPROFILE }}\\\\situation.exe\" agent_ids : \"{{ src_dir }}/agents.txt\" arch_map : x86_64 : \"amd64\" aarch64 : \"arm64\" armv5l : \"armv5l\" armv6l : \"armv6l\" armv7l : \"armv7l\" AMD64 : \"amd64\" tasks : - name : Gather required facts ansible.builtin.setup : gather_subset : - \"!all\" # only the min subset is collected - \"env\" - name : Copy Linux binary to target copy : src : \"{{ src_dir }}/{{ item }}\" dest : \"{{ dest_linux }}\" mode : \"0755\" loop : \"{{ lookup('fileglob', src_dir + '/*-' + arch_map[ansible_architecture] + '-linux', wantlist=True) | map('basename') | list }}\" when : ansible_system == \"Linux\" - name : Copy Windows binary to target win_copy : src : \"{{ src_dir }}/{{ item }}\" dest : \"{{ dest_windows }}\" loop : \"{{ lookup('fileglob', src_dir + '/*-' + arch_map[ansible_architecture] + '-windows.exe', wantlist=True) | map('basename') | list }}\" when : ansible_os_family == \"Windows\" - name : Set situation binary path set_fact : situation : \"{{ dest_windows if ansible_os_family == 'Windows' else dest_linux }}\" - name : Refresh agent id (Windows) win_command : \"{{ situation }} refresh-id\" when : ansible_os_family == \"Windows\" - name : Refresh agent id (Linux) command : \"{{ situation }} refresh-id\" when : ansible_system == \"Linux\" - name : Get agent id (Windows) win_command : \"{{ situation }} id\" when : ansible_os_family == \"Windows\" register : id_output_windows changed_when : false - name : Get agent id (Linux) command : \"{{ situation }} id\" when : ansible_system == \"Linux\" register : id_output_linux changed_when : false - name : Retrieve new agent id set_fact : agent_id : \"{{ (id_output_windows.stdout_lines[0]) if ansible_os_family == 'Windows' else (id_output_linux.stdout_lines[0]) }}\" - name : List all deployments delegate_to : localhost throttle : 1 lineinfile : line : \"{{ ansible_hostname }},{{ agent_id }}\" path : \"{{ agent_ids }}\" create : true situation.sh If you use the situation.sh platform you must authorize the deployed agents.","title":"Ansible"},{"location":"deploy/ansible/#situationsh","text":"If you use the situation.sh platform you must authorize the deployed agents.","title":"situation.sh"},{"location":"deploy/footprint/","text":"Here are two environment variables that can be used to limit the CPU and memory usage of the agent. It The GOMAXPROCS variable limits the number of operating system threads that can execute user-level Go code simultaneously 1 . Linux GOMAXPROCS = 1 ./situation Windows & { $env:GOMAXPROCS = \"1\" ; .\\ situation . exe } The GOMEMLIMIT variable sets a soft memory limit for the runtime. A zero limit or a limit that's lower than the amount of memory used by the Go runtime may cause the garbage collector to run nearly continuously. However, the application may still make progress 1 . Linux GOMEMLIMIT = 10MiB ./situation Windows & { $env:GOMEMLIMIT = \"10MiB\" ; .\\ situation . exe } See documentation . \u21a9 \u21a9","title":"Limiting footprint"},{"location":"deploy/scheduled_task/","text":"Once downloaded, the binary can install itself as a cronjob (Linux) or as a scheduled task (Windows). Why not a service? Situation should be run regularly but occasionally . It is not meant to run indefinitely in the background. Installing Here is an example that installs the scheduled task that triggers every day at midnight. Linux situation cron --task-start 00 :00:00 --db = db.sqlite Windows situation . exe task - -task-start 00 : 00 : 00 - -db = db . sqlite Important You need admin privileges to install the scheduled task. On Windows, it creates a SYSTEM task and on Linux, it writes the job to /etc/cron.d/situation . Any run parameters passed to the command line will be appended to the task. It means that if you run the command below, Linux situation cron --task-start 00 :00:00 --db = db.sqlite --tcp-scan-timeout = 5s Windows situation . exe task - -task-start 00 : 00 : 00 - -db = db . sqlite - -tcp-scan-timeout = 5s the binary will be run with the flags --db=db.sqlite and --tcp-scan-timeout=5s . Uninstalling The task can be removed with the --uninstall flag (privileges still required). Linux situation cron --uninstall Windows situation . exe task - -uninstall","title":"Scheduled task"},{"location":"deploy/scheduled_task/#installing","text":"Here is an example that installs the scheduled task that triggers every day at midnight. Linux situation cron --task-start 00 :00:00 --db = db.sqlite Windows situation . exe task - -task-start 00 : 00 : 00 - -db = db . sqlite Important You need admin privileges to install the scheduled task. On Windows, it creates a SYSTEM task and on Linux, it writes the job to /etc/cron.d/situation . Any run parameters passed to the command line will be appended to the task. It means that if you run the command below, Linux situation cron --task-start 00 :00:00 --db = db.sqlite --tcp-scan-timeout = 5s Windows situation . exe task - -task-start 00 : 00 : 00 - -db = db . sqlite - -tcp-scan-timeout = 5s the binary will be run with the flags --db=db.sqlite and --tcp-scan-timeout=5s .","title":"Installing"},{"location":"deploy/scheduled_task/#uninstalling","text":"The task can be removed with the --uninstall flag (privileges still required). Linux situation cron --uninstall Windows situation . exe task - -uninstall","title":"Uninstalling"},{"location":"deploy/sentry/","text":"The agent runs can be monitored through Sentry once you provide your sentry DSN to the --sentry flag. Linux situation run --sentry = \"https://6eb600b24dd5c42fb149cf84d2240bef@o458801043321e512.ingest.us.sentry.io/4509589955543040\" Windows situation . exe run -sentry = \"https://6eb600b24dd5c42fb149cf84d2240bef@o458801043321e512.ingest.us.sentry.io/4509589955543040\" The internal sentry client forward logs (not debug logs) in addition to fine-grained monitoring data (module level). It attaches the following metadata (not configurable). Metadata Value Example ServerName Agent ID (hex string format) fc097e65503cb3ad9eb8e10f5a617611 Release Agent version 0.20.0 Dist Agent commit d7fa41d254d75496200d6fe0c71b1b4bf13892b1","title":"Sentry"},{"location":"developer/","text":"The Situation project aims to be enriched by the community, and modules are definitely a good starting point for developers to contribute. Before detailing the internals of Situation, it is paramount to understand the overall spirit of the project. No user interaction : it means that the agent must run without configuration, without integration, without dependency. In some cases, we obviously need some extra information. In this project, the developer should code enough logic to guess what is missing. For instance, if you want to detect a database, you need to guess what could be its listening port. Fortunately, modules also provide data that could be useful for subsequent modules through the store . So developers should well define their dependencies to ease the workflow of their module. Basically, we should avoid to do twice the same thing. Security : yes it is hard to ensure at 100%. However, for this kind of project, we quickly feel like using exec.Command and other shortcuts that ease developers' job (but decrease security level). So, do not use exec.Command and do not use library that uses it. In a nutshell, we should keep in mind that this agent is likely to run with root privileges on critical systems. Key concepts Context-driven : modules receive a context.Context carrying the logger, storage, and agent ID. See modules for details. Relational storage : collected data is stored in a relational database (SQLite or PostgreSQL) via the Bun ORM . See store for details. Dependency resolution : the scheduler automatically orders module execution based on declared dependencies. See architecture for the big picture.","title":"Overview"},{"location":"developer/#key-concepts","text":"Context-driven : modules receive a context.Context carrying the logger, storage, and agent ID. See modules for details. Relational storage : collected data is stored in a relational database (SQLite or PostgreSQL) via the Bun ORM . See store for details. Dependency resolution : the scheduler automatically orders module execution based on declared dependencies. See architecture for the big picture.","title":"Key concepts"},{"location":"developer/architecture/","text":"The project is organized around a core library ( pkg/ ) and an agent application ( agent/ ). Package Overview Package Job agent Agent entrypoint and CLI subcommands (run, id, task, update, version) pkg/models Data structures representing discoverable entities (Machine, Application, NetworkInterface, etc.) pkg/modules All collection modules (plugins) pkg/store Database layer using Bun ORM (SQLite and PostgreSQL) pkg/utils Extra helpers Core Concepts The overall architecture is plugin-based. A scheduler resolves module dependencies and runs the available modules in order. Each module receives a context.Context that carries: logger : a logrus field logger scoped to the module storage : a BunStorage instance connected to a database (SQLite or PostgreSQL) agent : the agent identifier string All collected data is persisted via the store into a relational database.","title":"Architecture"},{"location":"developer/architecture/#package-overview","text":"Package Job agent Agent entrypoint and CLI subcommands (run, id, task, update, version) pkg/models Data structures representing discoverable entities (Machine, Application, NetworkInterface, etc.) pkg/modules All collection modules (plugins) pkg/store Database layer using Bun ORM (SQLite and PostgreSQL) pkg/utils Extra helpers","title":"Package Overview"},{"location":"developer/architecture/#core-concepts","text":"The overall architecture is plugin-based. A scheduler resolves module dependencies and runs the available modules in order. Each module receives a context.Context that carries: logger : a logrus field logger scoped to the module storage : a BunStorage instance connected to a database (SQLite or PostgreSQL) agent : the agent identifier string All collected data is persisted via the store into a relational database.","title":"Core Concepts"},{"location":"developer/contributing/","text":"When contributing to this project, please first discuss the change you wish to make via Github issue . The issue should be enough documented to well understand the bug or the requested feature. AI-Generated Code We do not accept pull requests that consist primarily of AI-generated code. Contributors must write and understand their own contributions. License All the code submitted in this project follows the [project LICENSE](https://github.com/situation-sh/situation/blob/main/LICENSE.md). PR Process Fork the project into your personal namespace (or group) on Github. Create a feature branch in your fork with naming <issue-id>-<lowercase-title-of-the-issue> . Make changes (code, docs...) Ensure the documentation is updated ( make docs ) Push the commits to your feature branch in your fork. Submit a pull request (PR) to the main branch in the main Github project. Pre-commit Hooks The project uses pre-commit for git hooks. Install them with: uv sync --all-extras uv run pre-commit install Coding Style Taken from moby/moby Unless explicitly stated, we follow all coding guidelines from the Go community. While some of these standards may seem arbitrary, they somehow seem to result in a solid, consistent codebase. It is possible that the code base does not currently comply with these guidelines. We are not looking for a massive PR that fixes this, since that goes against the spirit of the guidelines. All new contributions should make a best effort to clean up and make the code base better than they left it. Obviously, apply your best judgement. Remember, the goal here is to make the code base easier for humans to navigate and understand. Always keep that in mind when nudging others to comply. If you are having trouble getting into the mood of idiomatic Go, we recommend reading through Effective Go . The Go Blog is also a great resource. Drinking the kool-aid is a lot easier than going thirsty.","title":"Contributing"},{"location":"developer/contributing/#ai-generated-code","text":"We do not accept pull requests that consist primarily of AI-generated code. Contributors must write and understand their own contributions.","title":"AI-Generated Code"},{"location":"developer/contributing/#license","text":"All the code submitted in this project follows the [project LICENSE](https://github.com/situation-sh/situation/blob/main/LICENSE.md).","title":"License"},{"location":"developer/contributing/#pr-process","text":"Fork the project into your personal namespace (or group) on Github. Create a feature branch in your fork with naming <issue-id>-<lowercase-title-of-the-issue> . Make changes (code, docs...) Ensure the documentation is updated ( make docs ) Push the commits to your feature branch in your fork. Submit a pull request (PR) to the main branch in the main Github project.","title":"PR Process"},{"location":"developer/contributing/#pre-commit-hooks","text":"The project uses pre-commit for git hooks. Install them with: uv sync --all-extras uv run pre-commit install","title":"Pre-commit Hooks"},{"location":"developer/contributing/#coding-style","text":"Taken from moby/moby Unless explicitly stated, we follow all coding guidelines from the Go community. While some of these standards may seem arbitrary, they somehow seem to result in a solid, consistent codebase. It is possible that the code base does not currently comply with these guidelines. We are not looking for a massive PR that fixes this, since that goes against the spirit of the guidelines. All new contributions should make a best effort to clean up and make the code base better than they left it. Obviously, apply your best judgement. Remember, the goal here is to make the code base easier for humans to navigate and understand. Always keep that in mind when nudging others to comply. If you are having trouble getting into the mood of idiomatic Go, we recommend reading through Effective Go . The Go Blog is also a great resource. Drinking the kool-aid is a lot easier than going thirsty.","title":"Coding Style"},{"location":"developer/modules/","text":"Introduction A module is an independent piece of code that can be run during scan. Its job is merely to enrich the store. It is not fully independent as it may depend on previous modules (some module are likely to need data provided by others). To develop a module, create a new my_new_module.go source file in the pkg/modules/ directory. The structure of the module should look like the following snippet. package modules import ( \"context\" // ... ) func init () { registerModule ( & MyNewModule { attribute : \"defaultValue\" , }) } // Module definition ------------------------------------------------ type MyNewModule struct { BaseModule attribute string } // Name returns the name of the module func ( m * MyNewModule ) Name () string { return \"my-new-module\" } // Dependencies return the list of modules // required to run this one func ( m * MyNewModule ) Dependencies () [] string { return [] string { \"host-basic\" } } // Run does the job. It returns error only if it really // fails, i.e. it cannot be run (like privileges). // In the other cases, just log the errors func ( m * MyNewModule ) Run ( ctx context . Context ) error { // extract the logger and storage from the context logger := getLogger ( ctx , m ) storage := getStorage ( ctx ) // ... // do what you want // ... // but do not return error except if something // prevents the module to be run, just log them: // logger. // WithError(err). // Warn(\"something wrong but not critical happens\") // ... return nil } Naming You are free about the module naming, but obviously there are some constraints: the module name must be unique the name should describe what the module does (or the ecosystem, like \"docker\") If you want to create a module called \"awesome stuff\": its name (output of .Name() ) must be awesome-stuff the object that respects the Module interface must be AwesomeStuffModule the source file must be awesome_stuff.go Module interface A module must implement the Module interface described below. // Module is the generic module interface to implement plugins to // the agent type Module interface { Name () string Dependencies () [] string Run ( ctx context . Context ) error } The Name() outputs the unique name of the module. The Dependencies() returns the names of the modules required to start this module (prior information). The Run(ctx) function does the job. This function is called during the scan by the scheduler. The context.Context carries the logger, storage, and agent identifier. The function may have several interactions: config (get extra configuration data) logging (output some information about the run) store (retrieve/store collected data) BaseModule All modules should embed BaseModule : type MyNewModule struct { BaseModule // your fields here } Currently this object does not provide extra attribute/methods. But this is where we could inject ones in the future. Registration Modules must be registered via init() using the unexported registerModule function. This adds the module to the internal map. It panics if two modules share the same name. func init () { registerModule ( & MyNewModule {}) } Context The Run method receives a context.Context prepared by the scheduler. In this catch-all parameter, we provide all the runtile needs of the module. Currently, three helpers extract what you need: Helper Returns Description getLogger(ctx, m) logrus.FieldLogger Logger scoped to the module name getStorage(ctx) *store.BunStorage Database storage instance getAgent(ctx) string Agent identifier func ( m * MyNewModule ) Run ( ctx context . Context ) error { logger := getLogger ( ctx , m ) storage := getStorage ( ctx ) agent := getAgent ( ctx ) // ... } Configuration The configuration is managed by asiffer/puzzle . If your module needs configurable attributes, put them in the module struct with a default value and implement the Configurable interface defined in agent/config : type Configurable interface { Bind ( config * puzzle . Config ) error } Inside Bind , use the setDefault helper to register parameters: func ( m * MyNewModule ) Bind ( config * puzzle . Config ) error { return setDefault ( config , m , \"attribute\" , & m . Attribute , \"Custom attribute for my new module\" ) } The parameters are stored in the modules.module-name.* namespace and are automatically exposed as CLI flags. In your Run() function, access attributes directly through the pointer receiver: func ( m * MyNewModule ) Run ( ctx context . Context ) error { // access it directly attr := m . attribute // ... } Logging The logging is managed by logrus . To log some information, extract the logger from the context with getLogger . This returns a logger automatically scoped to the module name. func ( m * MyNewModule ) Run ( ctx context . Context ) error { logger := getLogger ( ctx , m ) // now you can use the classical methods logger . Debug ( \"Debug message\" ) logger . Info ( \"Info message\" ) logger . Warn ( \"Warning message\" ) logger . Error ( \"Error message\" ) // you should avoid logger.Panic to prevent the agent from crashing // ... } You should log collected data in a structured manner with logger.WithField : func ( m * MyNewModule ) Run ( ctx context . Context ) error { logger := getLogger ( ctx , m ) // ... logger . WithField ( \"hostname\" , hostname ). Debug ( \"Hostname found!\" ) } Big module case If your module is heavy you can store the implementation inside a sub-package and write a short interface in the modules directory. You may have the following layout: pkg/modules/ heavy.go heavy/ file1.go file2.go ... The heavy.go file may look like the following: package modules import ( \"context\" // load the sub-package heavy \"github.com/situation-sh/situation/pkg/modules/heavy\" ) type HeavyModule struct { BaseModule } func init () { registerModule ( & HeavyModule {}) } func ( m * HeavyModule ) Name () string { return \"heavy\" } func ( m * HeavyModule ) Dependencies () [] string { return [] string {} } func ( m * HeavyModule ) Run ( ctx context . Context ) error { logger := getLogger ( ctx , m ) storage := getStorage ( ctx ) // delegate to the sub-package return heavy . DoWork ( ctx , logger , storage ) } Documentation Documenting a module is mandatory. There are two things to do. The first thing is to document the Module object as follows: // MyNewModule retrieves data from ... // // It mainly depends on the following external library: // - ... // // On Windows, it collect data by calling... // On Linux, it reads ... type MyNewModule struct { BaseModule } One must have a synopsis (first line) and then some details about the module. One may include how data is collected with regards to the platform and also other relevant things (edge cases, libraries, privileges, options etc.) The second point is to fill some standard notes, as follows: // LINUX(MyNewModule) ok // WINDOWS(MyNewModule) ok // MACOS(MyNewModule) ? // ROOT(MyNewModule) no package modules The format of the note is given by the doc package. We use it as follows: <KEY>(<MODULE-NAME>) <VALUE> Currently there are 4 attributes to provide: LINUX , WINDOWS , MACOS and ROOT . Their corresponding values must be yes / ok (meaning \"supported\"), no (meaning \"not supported\"), or ? (meaning \"don't know\"). For ROOT , yes / ok means that root privileges are required","title":"Module"},{"location":"developer/modules/#introduction","text":"A module is an independent piece of code that can be run during scan. Its job is merely to enrich the store. It is not fully independent as it may depend on previous modules (some module are likely to need data provided by others). To develop a module, create a new my_new_module.go source file in the pkg/modules/ directory. The structure of the module should look like the following snippet. package modules import ( \"context\" // ... ) func init () { registerModule ( & MyNewModule { attribute : \"defaultValue\" , }) } // Module definition ------------------------------------------------ type MyNewModule struct { BaseModule attribute string } // Name returns the name of the module func ( m * MyNewModule ) Name () string { return \"my-new-module\" } // Dependencies return the list of modules // required to run this one func ( m * MyNewModule ) Dependencies () [] string { return [] string { \"host-basic\" } } // Run does the job. It returns error only if it really // fails, i.e. it cannot be run (like privileges). // In the other cases, just log the errors func ( m * MyNewModule ) Run ( ctx context . Context ) error { // extract the logger and storage from the context logger := getLogger ( ctx , m ) storage := getStorage ( ctx ) // ... // do what you want // ... // but do not return error except if something // prevents the module to be run, just log them: // logger. // WithError(err). // Warn(\"something wrong but not critical happens\") // ... return nil }","title":"Introduction"},{"location":"developer/modules/#naming","text":"You are free about the module naming, but obviously there are some constraints: the module name must be unique the name should describe what the module does (or the ecosystem, like \"docker\") If you want to create a module called \"awesome stuff\": its name (output of .Name() ) must be awesome-stuff the object that respects the Module interface must be AwesomeStuffModule the source file must be awesome_stuff.go","title":"Naming"},{"location":"developer/modules/#module-interface","text":"A module must implement the Module interface described below. // Module is the generic module interface to implement plugins to // the agent type Module interface { Name () string Dependencies () [] string Run ( ctx context . Context ) error } The Name() outputs the unique name of the module. The Dependencies() returns the names of the modules required to start this module (prior information). The Run(ctx) function does the job. This function is called during the scan by the scheduler. The context.Context carries the logger, storage, and agent identifier. The function may have several interactions: config (get extra configuration data) logging (output some information about the run) store (retrieve/store collected data)","title":"Module interface"},{"location":"developer/modules/#basemodule","text":"All modules should embed BaseModule : type MyNewModule struct { BaseModule // your fields here } Currently this object does not provide extra attribute/methods. But this is where we could inject ones in the future.","title":"BaseModule"},{"location":"developer/modules/#registration","text":"Modules must be registered via init() using the unexported registerModule function. This adds the module to the internal map. It panics if two modules share the same name. func init () { registerModule ( & MyNewModule {}) }","title":"Registration"},{"location":"developer/modules/#context","text":"The Run method receives a context.Context prepared by the scheduler. In this catch-all parameter, we provide all the runtile needs of the module. Currently, three helpers extract what you need: Helper Returns Description getLogger(ctx, m) logrus.FieldLogger Logger scoped to the module name getStorage(ctx) *store.BunStorage Database storage instance getAgent(ctx) string Agent identifier func ( m * MyNewModule ) Run ( ctx context . Context ) error { logger := getLogger ( ctx , m ) storage := getStorage ( ctx ) agent := getAgent ( ctx ) // ... }","title":"Context"},{"location":"developer/modules/#configuration","text":"The configuration is managed by asiffer/puzzle . If your module needs configurable attributes, put them in the module struct with a default value and implement the Configurable interface defined in agent/config : type Configurable interface { Bind ( config * puzzle . Config ) error } Inside Bind , use the setDefault helper to register parameters: func ( m * MyNewModule ) Bind ( config * puzzle . Config ) error { return setDefault ( config , m , \"attribute\" , & m . Attribute , \"Custom attribute for my new module\" ) } The parameters are stored in the modules.module-name.* namespace and are automatically exposed as CLI flags. In your Run() function, access attributes directly through the pointer receiver: func ( m * MyNewModule ) Run ( ctx context . Context ) error { // access it directly attr := m . attribute // ... }","title":"Configuration"},{"location":"developer/modules/#logging","text":"The logging is managed by logrus . To log some information, extract the logger from the context with getLogger . This returns a logger automatically scoped to the module name. func ( m * MyNewModule ) Run ( ctx context . Context ) error { logger := getLogger ( ctx , m ) // now you can use the classical methods logger . Debug ( \"Debug message\" ) logger . Info ( \"Info message\" ) logger . Warn ( \"Warning message\" ) logger . Error ( \"Error message\" ) // you should avoid logger.Panic to prevent the agent from crashing // ... } You should log collected data in a structured manner with logger.WithField : func ( m * MyNewModule ) Run ( ctx context . Context ) error { logger := getLogger ( ctx , m ) // ... logger . WithField ( \"hostname\" , hostname ). Debug ( \"Hostname found!\" ) }","title":"Logging"},{"location":"developer/modules/#big-module-case","text":"If your module is heavy you can store the implementation inside a sub-package and write a short interface in the modules directory. You may have the following layout: pkg/modules/ heavy.go heavy/ file1.go file2.go ... The heavy.go file may look like the following: package modules import ( \"context\" // load the sub-package heavy \"github.com/situation-sh/situation/pkg/modules/heavy\" ) type HeavyModule struct { BaseModule } func init () { registerModule ( & HeavyModule {}) } func ( m * HeavyModule ) Name () string { return \"heavy\" } func ( m * HeavyModule ) Dependencies () [] string { return [] string {} } func ( m * HeavyModule ) Run ( ctx context . Context ) error { logger := getLogger ( ctx , m ) storage := getStorage ( ctx ) // delegate to the sub-package return heavy . DoWork ( ctx , logger , storage ) }","title":"Big module case"},{"location":"developer/modules/#documentation","text":"Documenting a module is mandatory. There are two things to do. The first thing is to document the Module object as follows: // MyNewModule retrieves data from ... // // It mainly depends on the following external library: // - ... // // On Windows, it collect data by calling... // On Linux, it reads ... type MyNewModule struct { BaseModule } One must have a synopsis (first line) and then some details about the module. One may include how data is collected with regards to the platform and also other relevant things (edge cases, libraries, privileges, options etc.) The second point is to fill some standard notes, as follows: // LINUX(MyNewModule) ok // WINDOWS(MyNewModule) ok // MACOS(MyNewModule) ? // ROOT(MyNewModule) no package modules The format of the note is given by the doc package. We use it as follows: <KEY>(<MODULE-NAME>) <VALUE> Currently there are 4 attributes to provide: LINUX , WINDOWS , MACOS and ROOT . Their corresponding values must be yes / ok (meaning \"supported\"), no (meaning \"not supported\"), or ? (meaning \"don't know\"). For ROOT , yes / ok means that root privileges are required","title":"Documentation"},{"location":"developer/store/","text":"The Store is a relational database layer where modules persist collected data. It is implemented by the BunStorage type in pkg/store/ , built on top of the Bun ORM and supports both SQLite (embedded, default) and PostgreSQL (remote). Warning The store and the models may evolve. Changes to models (add/modify/remove attributes) can have a wide impact. Always check existing methods before writing raw queries. Usage Inside a module's Run function, the storage can be extracted from the context. func ( m * MyModule ) Run ( ctx context . Context ) error { storage := getStorage ( ctx ) // ... } Then, you can either use storage helpers to read/write the database or code your own query. // some helpers are diretly available host := storage . GetOrCreateHost ( ctx ) // Otherwise you can build your own queries // using the underlying Bun DB err := storage . DB (). NewUpdate (). Model (( * models . Machine )( nil )). Where ( \"id = ?\" , host . ID ). Set ( \"hostname = ?\" , hostname ). Returning ( \"*\" ). Scan ( ctx , host ) Info The modules are likely to build their own queries since they collect different things.","title":"Store"},{"location":"developer/store/#usage","text":"Inside a module's Run function, the storage can be extracted from the context. func ( m * MyModule ) Run ( ctx context . Context ) error { storage := getStorage ( ctx ) // ... } Then, you can either use storage helpers to read/write the database or code your own query. // some helpers are diretly available host := storage . GetOrCreateHost ( ctx ) // Otherwise you can build your own queries // using the underlying Bun DB err := storage . DB (). NewUpdate (). Model (( * models . Machine )( nil )). Where ( \"id = ?\" , host . ID ). Set ( \"hostname = ?\" , hostname ). Returning ( \"*\" ). Scan ( ctx , host ) Info The modules are likely to build their own queries since they collect different things.","title":"Usage"},{"location":"modules/","text":"Linux Windows Root required Name Summary Dependencies Status arp ARPModule reads internal ARP table to find network neighbors. ping chassis ChassisModule fills host chassis information host-basic docker DockerModule retrieves information about docker containers. host-network , tcp-scan dpkg DPKGModule reads package information from the dpkg package manager. host-basic , netstat fingerprint FingerprintModule attempts to match the local host against machines already discovered in the shared database. host-basic HostBasicModule retrieves basic information about the host: hostid, architecture, platform, distribution, version and uptime fingerprint host-cpu HostCPUModule retrieves host CPU info: model, vendor and the number of cores. host-basic host-disk HostDiskModule retrieves basic information about disk: name, model, size, type, controller and partitions. host-basic host-gpu HostGPUModule retrieves basic information about GPU: index, vendor and product name. host-basic host-network HostNetworkModule retrieves basic network information about the host. host-basic ja4 JA4Module attempts JA4, JA4S and JA4X fingerprinting tls local-users LocalUsersModule lists all local user accounts on the system. host-basic macvendor MACVendorModule resolves manufacturer from MAC addresses. arp msi MSIModule creates models.Packages instance from the windows registry host-basic netstat NetstatModule retrieves active connections. local-users , tcp-scan ping PingModule pings local networks to discover new hosts. host-network reverse-lookup ReverseLookupModule tries to get a hostname attached to a local IP address arp rpm RPMModule reads package information from the rpm package manager. host-basic , netstat saas SaaSModule identifies SaaS applications from discovered endpoints. tls , ja4 standard-protocol StandardProtocolModule fills standard protocol information for endpoints. netstat tcp-scan TCPScanModule tries to connect to neighbor TCP ports. arp tls TLSModule enriches TCP endpoints with TLS certificate information. tcp-scan , netstat zypper ZypperModule reads package information from the zypper package manager. host-basic , netstat","title":"Modules reference"},{"location":"modules/arp/","text":"ARPModule reads internal ARP table to find network neighbors. Details It does not send ARP requests but leverage the Ping module that is likely to update the local table. On Linux, it uses the Netlink API with the netlink library. On Windows, it calls GetIpNetTable2 . Dependencies Standard library context encoding/binary fmt net syscall time unsafe External github.com/vishvananda/netlink golang.org/x/sys/windows","title":"ARP"},{"location":"modules/arp/#details","text":"It does not send ARP requests but leverage the Ping module that is likely to update the local table. On Linux, it uses the Netlink API with the netlink library. On Windows, it calls GetIpNetTable2 .","title":"Details"},{"location":"modules/arp/#dependencies","text":"Standard library context encoding/binary fmt net syscall time unsafe External github.com/vishvananda/netlink golang.org/x/sys/windows","title":"Dependencies"},{"location":"modules/chassis/","text":"ChassisModule fills host chassis information Details Currently it only works under linux. It uses DBUS and the \"org.freedesktop.hostname1\" service to get the type of the chassis (like laptop, vm, desktop etc.) In the future it may rather rely on ghw but at that time it does not fully get the info on windows. Dependencies Standard library context fmt os External github.com/godbus/dbus/v5","title":"Chassis"},{"location":"modules/chassis/#details","text":"Currently it only works under linux. It uses DBUS and the \"org.freedesktop.hostname1\" service to get the type of the chassis (like laptop, vm, desktop etc.) In the future it may rather rely on ghw but at that time it does not fully get the info on windows.","title":"Details"},{"location":"modules/chassis/#dependencies","text":"Standard library context fmt os External github.com/godbus/dbus/v5","title":"Dependencies"},{"location":"modules/docker/","text":"DockerModule retrieves information about docker containers. Details It uses the official go client that performs HTTP queries either on port :2376 (on windows generally) or on UNIX sockets. We generally need some privileges to reads UNIX sockets, so it may require root privileges (the alternative is to belong to the docker group) Dependencies Standard library context fmt net runtime strings time External github.com/asiffer/puzzle github.com/docker/docker/api/types/container github.com/docker/docker/api/types/filters github.com/docker/docker/api/types/network github.com/docker/docker/client github.com/sirupsen/logrus","title":"Docker"},{"location":"modules/docker/#details","text":"It uses the official go client that performs HTTP queries either on port :2376 (on windows generally) or on UNIX sockets. We generally need some privileges to reads UNIX sockets, so it may require root privileges (the alternative is to belong to the docker group)","title":"Details"},{"location":"modules/docker/#dependencies","text":"Standard library context fmt net runtime strings time External github.com/asiffer/puzzle github.com/docker/docker/api/types/container github.com/docker/docker/api/types/filters github.com/docker/docker/api/types/network github.com/docker/docker/client github.com/sirupsen/logrus","title":"Dependencies"},{"location":"modules/dpkg/","text":"DPKGModule reads package information from the dpkg package manager. Details This module is relevant for distros that use dpkg, like debian, ubuntu and their derivatives. It only uses the standard library. It reads /var/log/dpkg.log and also files from /var/lib/dpkg/info/ . Dependencies Standard library bufio context errors fmt os path/filepath strings time External","title":"DPKG"},{"location":"modules/dpkg/#details","text":"This module is relevant for distros that use dpkg, like debian, ubuntu and their derivatives. It only uses the standard library. It reads /var/log/dpkg.log and also files from /var/lib/dpkg/info/ .","title":"Details"},{"location":"modules/dpkg/#dependencies","text":"Standard library bufio context errors fmt os path/filepath strings time External","title":"Dependencies"},{"location":"modules/fingerprint/","text":"FingerprintModule attempts to match the local host against machines already discovered in the shared database. Details This module is critical for multi-agent deployments where Agent A may have discovered Host B remotely (via ARP, ping, TCP scan), and later Agent B starts on Host B. Without fingerprinting, Agent B would create a duplicate machine entry instead of recognizing itself. Matching strategy: Agent UUID match \u2192 definitive (reconnection case) HostID (system UUID) match \u2192 definitive Fuzzy matching on MAC/IP/hostname with weighted scores The module runs before any other module (no dependencies) to ensure the host machine is correctly identified before other modules populate it. Dependencies Standard library context fmt net strings External github.com/shirou/gopsutil/v4/host","title":"Fingerprint"},{"location":"modules/fingerprint/#details","text":"This module is critical for multi-agent deployments where Agent A may have discovered Host B remotely (via ARP, ping, TCP scan), and later Agent B starts on Host B. Without fingerprinting, Agent B would create a duplicate machine entry instead of recognizing itself. Matching strategy: Agent UUID match \u2192 definitive (reconnection case) HostID (system UUID) match \u2192 definitive Fuzzy matching on MAC/IP/hostname with weighted scores The module runs before any other module (no dependencies) to ensure the host machine is correctly identified before other modules populate it.","title":"Details"},{"location":"modules/fingerprint/#dependencies","text":"Standard library context fmt net strings External github.com/shirou/gopsutil/v4/host","title":"Dependencies"},{"location":"modules/host_basic/","text":"HostBasicModule retrieves basic information about the host: hostid, architecture, platform, distribution, version and uptime Details It heavily relies on the gopsutil library. Data Linux Windows hostname uname syscall GetComputerNameExW call arch uname syscall GetNativeSystemInfo call platform runtime.GOOS variable runtime.GOOS variable distribution scanning /etc/*-release files HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion* register keys distribution version scanning /etc/*-release files HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion* register keys hostid reading /sys/class/dmi/id/product_uuid , /etc/machine-id or /proc/sys/kernel/random/boot_id HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Cryptography\\MachineGuid register key uptime sysinfo syscall GetTickCount64 call Dependencies Standard library context fmt os External github.com/shirou/gopsutil/v4/host","title":"HostBasic"},{"location":"modules/host_basic/#details","text":"It heavily relies on the gopsutil library. Data Linux Windows hostname uname syscall GetComputerNameExW call arch uname syscall GetNativeSystemInfo call platform runtime.GOOS variable runtime.GOOS variable distribution scanning /etc/*-release files HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion* register keys distribution version scanning /etc/*-release files HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion* register keys hostid reading /sys/class/dmi/id/product_uuid , /etc/machine-id or /proc/sys/kernel/random/boot_id HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Cryptography\\MachineGuid register key uptime sysinfo syscall GetTickCount64 call","title":"Details"},{"location":"modules/host_basic/#dependencies","text":"Standard library context fmt os External github.com/shirou/gopsutil/v4/host","title":"Dependencies"},{"location":"modules/host_cpu/","text":"HostCPUModule retrieves host CPU info: model, vendor and the number of cores. Details It heavily relies on the gopsutil library. On Linux, it reads /proc/cpuinfo . On Windows it performs the win32_Processor WMI request On windows 11, the local user account must have administrator permissions (it does not mean it must be run as root). Dependencies Standard library context fmt strconv External github.com/shirou/gopsutil/v4/cpu","title":"HostCPU"},{"location":"modules/host_cpu/#details","text":"It heavily relies on the gopsutil library. On Linux, it reads /proc/cpuinfo . On Windows it performs the win32_Processor WMI request On windows 11, the local user account must have administrator permissions (it does not mean it must be run as root).","title":"Details"},{"location":"modules/host_cpu/#dependencies","text":"Standard library context fmt strconv External github.com/shirou/gopsutil/v4/cpu","title":"Dependencies"},{"location":"modules/host_disk/","text":"HostDiskModule retrieves basic information about disk: name, model, size, type, controller and partitions. Details It heavily relies on the ghw library. On Windows, it uses WMI requests: ps1 SELECT Caption, CreationClassName, Description, DeviceID, FileSystem, FreeSpace, Name, Size, SystemName FROM Win32_LogicalDisk ps1 SELECT DeviceId, MediaType FROM MSFT_PhysicalDisk ps1 SELECT Access, BlockSize, Caption, CreationClassName, Description, DeviceID, DiskIndex, Index, Name, Size, SystemName, Type FROM Win32_DiskPartition ps1 SELECT Antecedent, Dependent FROM Win32_LogicalDiskToPartition On Linux, it reads /sys/block/$DEVICE/** files. On windows 11, the local user account must have administrator permissions (it does not mean it must be run as root). Dependencies Standard library context fmt External github.com/jaypipes/ghw","title":"HostDisk"},{"location":"modules/host_disk/#details","text":"It heavily relies on the ghw library. On Windows, it uses WMI requests: ps1 SELECT Caption, CreationClassName, Description, DeviceID, FileSystem, FreeSpace, Name, Size, SystemName FROM Win32_LogicalDisk ps1 SELECT DeviceId, MediaType FROM MSFT_PhysicalDisk ps1 SELECT Access, BlockSize, Caption, CreationClassName, Description, DeviceID, DiskIndex, Index, Name, Size, SystemName, Type FROM Win32_DiskPartition ps1 SELECT Antecedent, Dependent FROM Win32_LogicalDiskToPartition On Linux, it reads /sys/block/$DEVICE/** files. On windows 11, the local user account must have administrator permissions (it does not mean it must be run as root).","title":"Details"},{"location":"modules/host_disk/#dependencies","text":"Standard library context fmt External github.com/jaypipes/ghw","title":"Dependencies"},{"location":"modules/host_gpu/","text":"HostGPUModule retrieves basic information about GPU: index, vendor and product name. Details It heavily relies on ghw . On Linux it reads /sys/class/drm/ folder. On Windows, it performs the following WMI query: ps1 SELECT Caption, CreationClassName, Description, DeviceID, Manufacturer, Name, PNPClass, PNPDeviceID FROM Win32_PnPEntity On windows 11, the local user account must have administrator permissions (it does not mean it must be run as root). Dependencies Standard library context fmt External github.com/jaypipes/ghw","title":"HostGPU"},{"location":"modules/host_gpu/#details","text":"It heavily relies on ghw . On Linux it reads /sys/class/drm/ folder. On Windows, it performs the following WMI query: ps1 SELECT Caption, CreationClassName, Description, DeviceID, Manufacturer, Name, PNPClass, PNPDeviceID FROM Win32_PnPEntity On windows 11, the local user account must have administrator permissions (it does not mean it must be run as root).","title":"Details"},{"location":"modules/host_gpu/#dependencies","text":"Standard library context fmt External github.com/jaypipes/ghw","title":"Dependencies"},{"location":"modules/host_network/","text":"HostNetworkModule retrieves basic network information about the host. Details It uses the net standard library to grab interfaces along with their name, MAC address, IP addresses (IPv4 and IPv6), subnet masks and go-netroute for gateway detection. On Linux, it uses the Netlink API. On Windows, it calls GetAdaptersAddresses . Virtual interfaces (veth, qemu) are filtered out. The module also creates subnetwork records and links each network interface to its subnets. Dependencies Standard library context fmt net strings External github.com/libp2p/go-netroute","title":"HostNetwork"},{"location":"modules/host_network/#details","text":"It uses the net standard library to grab interfaces along with their name, MAC address, IP addresses (IPv4 and IPv6), subnet masks and go-netroute for gateway detection. On Linux, it uses the Netlink API. On Windows, it calls GetAdaptersAddresses . Virtual interfaces (veth, qemu) are filtered out. The module also creates subnetwork records and links each network interface to its subnets.","title":"Details"},{"location":"modules/host_network/#dependencies","text":"Standard library context fmt net strings External github.com/libp2p/go-netroute","title":"Dependencies"},{"location":"modules/ja4/","text":"JA4Module attempts JA4, JA4S and JA4X fingerprinting Details For technical details you look at https://github.com/FoxIO-LLC/ja4/blob/main/technical_details/README.md It first look at TLS endpoints (given by the TLS module ) and then tries to connect to them, collecting then JA4, JA4S and JA4X fingerprints. Dependencies Standard library context crypto/sha256 crypto/tls crypto/x509 crypto/x509/pkix encoding/asn1 encoding/binary encoding/hex fmt net slices strings time External github.com/sirupsen/logrus","title":"JA4"},{"location":"modules/ja4/#details","text":"For technical details you look at https://github.com/FoxIO-LLC/ja4/blob/main/technical_details/README.md It first look at TLS endpoints (given by the TLS module ) and then tries to connect to them, collecting then JA4, JA4S and JA4X fingerprints.","title":"Details"},{"location":"modules/ja4/#dependencies","text":"Standard library context crypto/sha256 crypto/tls crypto/x509 crypto/x509/pkix encoding/asn1 encoding/binary encoding/hex fmt net slices strings time External github.com/sirupsen/logrus","title":"Dependencies"},{"location":"modules/local_users/","text":"LocalUsersModule lists all local user accounts on the system. Details On Linux , the module reads /etc/passwd to enumerate user entries. Each UID is then resolved through the standard os/user library to retrieve the full user details. On Windows , the module calls the Win32 NetUserEnum API (from netapi32.dll ) to enumerate local accounts filtered to normal user accounts. Each username is then resolved with os/user.Lookup , and the user's domain is determined by converting the SID via LookupAccountSid . The collected users are stored in the database with an upsert strategy based on (machine_id, uid) . Dependencies Standard library bufio context fmt os os/user strings syscall unsafe External golang.org/x/sys/windows","title":"LocalUsers"},{"location":"modules/local_users/#details","text":"On Linux , the module reads /etc/passwd to enumerate user entries. Each UID is then resolved through the standard os/user library to retrieve the full user details. On Windows , the module calls the Win32 NetUserEnum API (from netapi32.dll ) to enumerate local accounts filtered to normal user accounts. Each username is then resolved with os/user.Lookup , and the user's domain is determined by converting the SID via LookupAccountSid . The collected users are stored in the database with an upsert strategy based on (machine_id, uid) .","title":"Details"},{"location":"modules/local_users/#dependencies","text":"Standard library bufio context fmt os os/user strings syscall unsafe External golang.org/x/sys/windows","title":"Dependencies"},{"location":"modules/macvendor/","text":"MACVendorModule resolves manufacturer from MAC addresses. Details It uses a built-in lookup table of IEEE OUI assignments (generated from the official IEEE OUI registry) to match the first 3 octets of each MAC address to a vendor name. The module queries all network interfaces that have a MAC address but no vendor yet, and updates them in bulk. Dependencies Standard library context fmt External","title":"MAC Vendor"},{"location":"modules/macvendor/#details","text":"It uses a built-in lookup table of IEEE OUI assignments (generated from the official IEEE OUI registry) to match the first 3 octets of each MAC address to a vendor name. The module queries all network interfaces that have a MAC address but no vendor yet, and updates them in bulk.","title":"Details"},{"location":"modules/macvendor/#dependencies","text":"Standard library context fmt External","title":"Dependencies"},{"location":"modules/msi/","text":"MSIModule creates models.Packages instance from the windows registry Details For system-wide apps, it looks at HKLM/SOFTWARE/Microsoft/Windows/CurrentVersion/Uninstall/* and HKLM/WOW6432Node/SOFTWARE/Microsoft/Windows/CurrentVersion/Uninstall/* for 32bits apps. For user-specific apps: HKCU/SOFTWARE/Microsoft/Windows/CurrentVersion/Uninstall/* . Dependencies Standard library context fmt io/fs os path/filepath strings time External github.com/sirupsen/logrus golang.org/x/sys/windows/registry","title":"MSI"},{"location":"modules/msi/#details","text":"For system-wide apps, it looks at HKLM/SOFTWARE/Microsoft/Windows/CurrentVersion/Uninstall/* and HKLM/WOW6432Node/SOFTWARE/Microsoft/Windows/CurrentVersion/Uninstall/* for 32bits apps. For user-specific apps: HKCU/SOFTWARE/Microsoft/Windows/CurrentVersion/Uninstall/* .","title":"Details"},{"location":"modules/msi/#dependencies","text":"Standard library context fmt io/fs os path/filepath strings time External github.com/sirupsen/logrus golang.org/x/sys/windows/registry","title":"Dependencies"},{"location":"modules/netstat/","text":"NetstatModule retrieves active connections. Details It enumerates TCP, UDP, TCP6 and UDP6 sockets to discover listening endpoints, running applications (with PID and command line), and network flows between them. It must be run as root on Linux to retrieve PID/process information; without these data it is hard to build reliable links between open ports and programs. This module is then able to create flows between applications according to the tuple (src, srcport, dst, dstport). On Windows, the privileges are not checked. So the module is always run. Dependencies Standard library context fmt os os/user runtime slices strings External github.com/cakturk/go-netstat/netstat","title":"Netstat"},{"location":"modules/netstat/#details","text":"It enumerates TCP, UDP, TCP6 and UDP6 sockets to discover listening endpoints, running applications (with PID and command line), and network flows between them. It must be run as root on Linux to retrieve PID/process information; without these data it is hard to build reliable links between open ports and programs. This module is then able to create flows between applications according to the tuple (src, srcport, dst, dstport). On Windows, the privileges are not checked. So the module is always run.","title":"Details"},{"location":"modules/netstat/#dependencies","text":"Standard library context fmt os os/user runtime slices strings External github.com/cakturk/go-netstat/netstat","title":"Dependencies"},{"location":"modules/ping/","text":"PingModule pings local networks to discover new hosts. Details The module relies on pro-bing library. A single ping attempt is made on every host of the local networks (the host may belong to several networks). Only IPv4 networks with prefix length >=20 are treated. The ping timeout is hardset to 300ms. Dependencies Standard library context encoding/binary fmt net os sync syscall time unsafe External github.com/asiffer/puzzle github.com/sirupsen/logrus golang.org/x/net/icmp golang.org/x/net/ipv4 golang.org/x/sys/windows","title":"Ping"},{"location":"modules/ping/#details","text":"The module relies on pro-bing library. A single ping attempt is made on every host of the local networks (the host may belong to several networks). Only IPv4 networks with prefix length >=20 are treated. The ping timeout is hardset to 300ms.","title":"Details"},{"location":"modules/ping/#dependencies","text":"Standard library context encoding/binary fmt net os sync syscall time unsafe External github.com/asiffer/puzzle github.com/sirupsen/logrus golang.org/x/net/icmp golang.org/x/net/ipv4 golang.org/x/sys/windows","title":"Dependencies"},{"location":"modules/reverse_lookup/","text":"ReverseLookupModule tries to get a hostname attached to a local IP address Details It basically calls net.LookupAddr that uses the host resolver to perform a reverse lookup for the given addresses. Dependencies Standard library context fmt net strings External","title":"ReverseLookup"},{"location":"modules/reverse_lookup/#details","text":"It basically calls net.LookupAddr that uses the host resolver to perform a reverse lookup for the given addresses.","title":"Details"},{"location":"modules/reverse_lookup/#dependencies","text":"Standard library context fmt net strings External","title":"Dependencies"},{"location":"modules/rpm/","text":"RPMModule reads package information from the rpm package manager. Details This module is relevant for distros that use rpm, like fedora, redhat and their derivatives. It uses an sqlite client because of the way rpm works. It tries to read the rpm database: /var/lib/rpm/rpmdb.sqlite . Otherwise, it will try to find the rpmdb.sqlite file inside /usr/lib . Dependencies Standard library bytes context database/sql encoding/binary fmt io/fs path path/filepath unicode/utf8 External github.com/hashicorp/go-version github.com/sirupsen/logrus modernc.org/sqlite","title":"RPM"},{"location":"modules/rpm/#details","text":"This module is relevant for distros that use rpm, like fedora, redhat and their derivatives. It uses an sqlite client because of the way rpm works. It tries to read the rpm database: /var/lib/rpm/rpmdb.sqlite . Otherwise, it will try to find the rpmdb.sqlite file inside /usr/lib .","title":"Details"},{"location":"modules/rpm/#dependencies","text":"Standard library bytes context database/sql encoding/binary fmt io/fs path path/filepath unicode/utf8 External github.com/hashicorp/go-version github.com/sirupsen/logrus modernc.org/sqlite","title":"Dependencies"},{"location":"modules/saas/","text":"SaaSModule identifies SaaS applications from discovered endpoints. Details For each endpoint that has not been classified yet, the module runs a set of pluggable detectors. Each detector matches endpoints by TLS DNS name suffixes or IP address ranges to identify known SaaS providers (e.g. GitHub, Outlook, Teams, SharePoint, Datadog, Sentry, Elastic, Anthropic). The first matching detector wins and the SaaS name is stored on the endpoint. Dependencies Standard library context errors fmt net strings External github.com/asiffer/puzzle","title":"SaaS"},{"location":"modules/saas/#details","text":"For each endpoint that has not been classified yet, the module runs a set of pluggable detectors. Each detector matches endpoints by TLS DNS name suffixes or IP address ranges to identify known SaaS providers (e.g. GitHub, Outlook, Teams, SharePoint, Datadog, Sentry, Elastic, Anthropic). The first matching detector wins and the SaaS name is stored on the endpoint.","title":"Details"},{"location":"modules/saas/#dependencies","text":"Standard library context errors fmt net strings External github.com/asiffer/puzzle","title":"Dependencies"},{"location":"modules/standard_protocol/","text":"StandardProtocolModule fills standard protocol information for endpoints. Details Dependencies Standard library context fmt strings External github.com/uptrace/bun github.com/uptrace/bun/dialect","title":"StandardProtocol"},{"location":"modules/standard_protocol/#details","text":"","title":"Details"},{"location":"modules/standard_protocol/#dependencies","text":"Standard library context fmt strings External github.com/uptrace/bun github.com/uptrace/bun/dialect","title":"Dependencies"},{"location":"modules/tcp_scan/","text":"TCPScanModule tries to connect to neighbor TCP ports. Details The module only uses the Go standard library. A TCP connect is performed on the NMAP top 1000 ports . These connection attempts are made concurrently against the hosts previously found. The connections have a 500ms timeout. Dependencies Standard library context fmt net strings time External github.com/asiffer/puzzle","title":"TCP Scan"},{"location":"modules/tcp_scan/#details","text":"The module only uses the Go standard library. A TCP connect is performed on the NMAP top 1000 ports . These connection attempts are made concurrently against the hosts previously found. The connections have a 500ms timeout.","title":"Details"},{"location":"modules/tcp_scan/#dependencies","text":"Standard library context fmt net strings time External github.com/asiffer/puzzle","title":"Dependencies"},{"location":"modules/tls/","text":"TLSModule enriches TCP endpoints with TLS certificate information. Details It connects to endpoints on well-known TLS ports (HTTPS, IMAPS, LDAPS, etc.) and performs a TLS handshake to extract the leaf certificate. For each certificate it collects: subject, issuer, validity period, serial number, signature and public key algorithms, SHA-1/SHA-256 fingerprints, and DNS names. The module only uses the Go standard library. Currently it only supports TLS over TCP. Dependencies Standard library context crypto/sha1 crypto/sha256 crypto/tls encoding/hex fmt net External github.com/sirupsen/logrus github.com/uptrace/bun","title":"TLS"},{"location":"modules/tls/#details","text":"It connects to endpoints on well-known TLS ports (HTTPS, IMAPS, LDAPS, etc.) and performs a TLS handshake to extract the leaf certificate. For each certificate it collects: subject, issuer, validity period, serial number, signature and public key algorithms, SHA-1/SHA-256 fingerprints, and DNS names. The module only uses the Go standard library. Currently it only supports TLS over TCP.","title":"Details"},{"location":"modules/tls/#dependencies","text":"Standard library context crypto/sha1 crypto/sha256 crypto/tls encoding/hex fmt net External github.com/sirupsen/logrus github.com/uptrace/bun","title":"Dependencies"},{"location":"modules/users/","text":"linux: true windows: true macos: false root: false title: LocalUsers summary: \"Reads package information from the dpkg package manager.\" date: 2026-02-02 filename: users.go std_imports: bufio context fmt os os/user strings syscall unsafe imports: golang.org/x/sys/windows LocalUsersModule reads package information from the dpkg package manager. Details This module is relevant for distros that use dpkg, like debian, ubuntu and their derivatives. It only uses the standard library. It reads /var/log/dpkg.log and also files from /var/lib/dpkg/info/ . Dependencies Standard library External","title":"Users"},{"location":"modules/users/#details","text":"This module is relevant for distros that use dpkg, like debian, ubuntu and their derivatives. It only uses the standard library. It reads /var/log/dpkg.log and also files from /var/lib/dpkg/info/ .","title":"Details"},{"location":"modules/users/#dependencies","text":"Standard library External","title":"Dependencies"},{"location":"modules/zypper/","text":"ZypperModule reads package information from the zypper package manager. Details This module is relevant for distros that use zypper, like suse and their derivatives. It uses go-rpmdb . It reads /var/lib/rpm/Packages.db . Dependencies Standard library context External github.com/knqyf263/go-rpmdb/pkg","title":"Zypper"},{"location":"modules/zypper/#details","text":"This module is relevant for distros that use zypper, like suse and their derivatives. It uses go-rpmdb . It reads /var/lib/rpm/Packages.db .","title":"Details"},{"location":"modules/zypper/#dependencies","text":"Standard library context External github.com/knqyf263/go-rpmdb/pkg","title":"Dependencies"}]}